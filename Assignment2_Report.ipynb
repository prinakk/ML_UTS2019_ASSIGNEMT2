{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinakk/ML_UTS2019_ASSIGNEMT2/blob/master/Assignment2_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9_a4nBKrLjz",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/prinakk/ML_UTS2019_ASSIGNEMT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BvkX5gSeRCR",
        "colab_type": "text"
      },
      "source": [
        "Assignment 2: Insurance claim prediction \n",
        "\n",
        "By\n",
        "\n",
        "Prina Kamakotti:13171780 Aditya Bamidipati Lakshmana Murthy:13133638\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAciGF02rwQy",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "##Problem and its significance:\n",
        "\n",
        "The data collected for our project is from Porto Seguro, a leading insurance company in Brazil. They have used machine learning to make insurance claim predictions, using the same data. The challenge presented here is to build a model that will predict the customer who is most likely to make a claim in the next year. \n",
        "\n",
        "Imagine a risky driver with a bad driving history, makes a claim year after year. This is grounds for increasing his/her insurance price for the next year. The prediction model's accuracy makes sure that this increase in price is reflected only on the bad-driver. If the accuracy is low, drivers with relatively fewer claims or less bad driving history might have their insurance price increased. This inaccuracy is the major concerning problem we are trying to deal with in this project.\n",
        " \n",
        "The use of predictions with better accuracy will help both the customers, as well as the insurance company.  Due to the drawback of more expenses for safe drivers and lower costs for risky ones, in effect, the prices are also highly reflected on the insurance company's profit.  \n",
        "\n",
        "With accurate predictions, they're used to fine-tune the prices for new and existing customers, helping to give more accessible coverage to drivers and with fair prices to every customer. This would lead to better profits for the company as a whole.       \n",
        "\n",
        "Due to the need for protecting the customer's information, the data has unlabelled features, which is a challenge on its own. There are around 595k records with each record having approximately 57 features. We use random forest and xg-boost classifiers with k-fold partitioning for model prediction.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh-CcWw-slNk",
        "colab_type": "text"
      },
      "source": [
        "#Exploration\n",
        "\n",
        "The data in question is from Kaggle, upoloaded by Porto Seguro. The data has 595,213 row data values. Each value has 57 features which are undefined and a 0/1 target column value which decides whether a claim is made or not. It can be seen that, 21,694 values have target association as 1 and the remaining as 0.   \n",
        "The test data has 892,816 rows and has one less feature than the training data, which is 56. The excluded feature is the target, which needs to be found. \n",
        "We have four data types represented in the dataset. They are:\n",
        "\n",
        "1.   Nominal which has 15 attributes\n",
        "2.   Ordinal with 15 attributes\n",
        "3.   Binary has 18 attributes and \n",
        "4.   Interval with 11.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We cannot rely on description to feature select and also due to the features being unlabeled, we cannot fit them as needed. These are problems we have identified. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTwHt_zY6hwj",
        "colab_type": "text"
      },
      "source": [
        "#Visualization\n",
        "\n",
        "Metadata: \n",
        "To ensure that data management is sufficient, we have stored the meta information of the attributes in a dataframe. This is helpful during visualising specific attributes for analysis and modelling. The attributes will be stored under: role, keep, level and dtype.\n",
        "\n",
        "After storing the metadata information, we have correlated the training set between level and keep of the metadata. This correlation has been visulaized using a heatmap. \n",
        "We then represent the features found with the maximum correlation on a scatterplot, from which their relationships can be strongly observed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O42lbi_n8Aev",
        "colab_type": "text"
      },
      "source": [
        "#Data Pre-processing\n",
        "\n",
        "In the dataset, we observe that there are a few missing values. These values are represented by -1, which the model determines to be a value instead of null. Hence, we use 'NaN' to represent the missing values. This improvisation made the model slightly more efficent than with the default values. \n",
        "\n",
        "How this was solved:\n",
        "\n",
        "First, we displayed all the features which had a missing value. These features were correlated using a heatmap and dendogram, where the depth of the dendogram indicated the correlation between the features. \n",
        "\n",
        "Next, through this viz, we came to a conclusion that the features ps_car_01_cat and ps_ind_04_cat are highly correlated, since their correlation is observed to be 1, they are colinear.\n",
        "\n",
        "Because of this correlation, we have concluded that they might be the same attributes. Even if the missing values are replaced by NaN, the features won't affect the model efficency.\n",
        "\n",
        "Sampling:\n",
        "\n",
        "Since, we have discovered that the class atrribute, target, is highly unbalanced, we have performed a SMOTE operation on the class attribute. What this does is, it oversamples the minority class to bring some balance to the class attribute. \n",
        "\n",
        "In our dataset, we have observed the minority class to be:\n",
        "\n",
        "\n",
        "1.   1 which has 21694 values and total of 3.64% of the class attribute.\n",
        "2.   0 has 573518 values and total of 96.36% of the class attribute.\n",
        "\n",
        "After SMOTE, we have oversampled the minority class '0' to a percentage of 10% and have observed that the efficency of the model drastically reduces. Since its unbalanced, we should not rely on accuracy of model but on the f-score and the like.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAbbaFaCA_o-",
        "colab_type": "text"
      },
      "source": [
        "#Data Modelling and Analysis\n",
        "\n",
        "Feature Selection:\n",
        "\n",
        "Since the machine learning model is based on the features, it is crucial to select the features that contribute most to the prediction. To relaize this, we have used a random forest model to classify the features according to their importance and have ranked the top ten based on the classification.\n",
        "\n",
        "After this, we have removed the features which are found to have their importance less than 0.01\n",
        "\n",
        "\n",
        "Training the model with training dataset:\n",
        "\n",
        "Split the training data. To achieve this, we have used the K-fold cross validation method. This split the training data into three folds. This method is a re-sampling procedure to evaluate and train the machine learning model.\n",
        "\n",
        "Model 1 and Model 2 comparative study:\n",
        "\n",
        "RandomForest:\n",
        "Defining the machine learning model:\n",
        "\n",
        "We have used a randomforest classifier and we've trained this model using the cross validation method. Further, we have found in the three validations, that the model scored 0.62460 in the first, 0.59937 in the second fold and in the final one 0.539.\n",
        "\n",
        "Testing the model with test data:\n",
        "\n",
        "The entire test data is then given to the trained model as input. The model then predicts the target attribute for the test data. The predicted out is finally evaluated with the actual results on Kaggle. Evaluation results: .\n",
        "\n",
        "XGB:\n",
        "Defining the model:\n",
        "\n",
        "Using the XGB classifier, we've followed the same cross validation method to train the model. In the first fold we observed a score of  , in the second the score was and the final score was .\n",
        "\n",
        "With test data:\n",
        "\n",
        "The test data as a whole is again the input, the only change is the classifer. The model predicts the class attribute - target and this prediction is evaluated on Kaggle. The evaluation output is:  .\n",
        "\n",
        "The Kaggle output is attached.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Q0qb-vH8jV",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "\n"
      ]
    }
  ]
}